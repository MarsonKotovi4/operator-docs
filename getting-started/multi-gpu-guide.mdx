---
title: Multi-GPU Docker Deployment Guide
description: How to run multiple Inference.net nodes on a multi-GPU machine using Docker.
---

# Multi-GPU Docker Deployment Guide

<Note>  
This guide is intended for advanced users who want to run multiple Inference.net nodes on a machine with multiple GPUs.
</Note>

## Prerequisites

- NVIDIA GPU drivers installed and working  
- NVIDIA Container Toolkit installed (`nvidia-container-toolkit`)  
- Docker >= 20.10 with `--gpus` support  
- Multiple GPU devices available  

## Running Multiple Containers with a Loop

You can easily run multiple Inference.net containers, one per GPU, using this loop:

```bash
for i in {0..5}; do 
  sudo docker run -d \
    --name inference-gpu-$i \
    --pull=always \
    --gpus device=$i \
    --restart=always \
    -v ~/.inference_gpu_$i:/root/.inference \
    inferencedevnet/amd64-nvidia-inference-node:latest \
    --code <YOUR_NODE_CODE_HERE>
done
```

## Notes

- Replace `<YOUR_NODE_CODE_HERE>` with your **node code** from the [Inference.net dashboard](https://devnet.inference.net/register).
- The loop runs **one container per GPU**. Adjust `{0..5}` according to the number of GPUs on your machine.
- Containers will be named `inference-gpu-0`, `inference-gpu-1`, etc.
- Each container will use a separate `.inference_gpu_$i` folder to avoid conflicts.

## Monitoring Containers

Check container logs:

```bash
docker logs -f inference-gpu-0
```

List running containers:

```bash
docker ps
```

Stop all containers:

```bash
docker stop $(docker ps -q --filter "name=inference-gpu-")
```

Remove all containers:

```bash
docker rm $(docker ps -aq --filter "name=inference-gpu-")
```

<CardGroup cols={2}>
  <Card title="Hardware Requirements" icon="microchip" href="/getting-started/hardware">
    Check out the list of officially supported hardware
  </Card>
  <Card title="Join Community" icon="discord" href="https://discord.gg/kuzco">
    Join the Inference.net Discord for more information and support
  </Card>
</CardGroup>
